{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8HQsAhz9j3kA",
    "outputId": "5d46ef7b-be38-4e33-e760-a46876c04bef"
   },
   "outputs": [],
   "source": [
    "# !git clone \"https://github.com/SriRamGovardhanam/wastedata-Mask_RCNN-multiple-classes.git\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kwn4yc5MksM_"
   },
   "outputs": [],
   "source": [
    "#!rm -rf /content/mrcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "0QqBELhkkOgz",
    "outputId": "beab45fb-ff1c-48e7-ede0-1774435bb05e"
   },
   "outputs": [],
   "source": [
    "# import shutil, os\n",
    "# def copytree(src = 'wastedata-Mask_RCNN-multiple-classes/main/Mask_RCNN', dst = '', symlinks=False, ignore=None):\n",
    "#     try:\n",
    "#         shutil.rmtree('/.ipynb_checkpoints')\n",
    "#     except:\n",
    "#         pass\n",
    "#     for item in os.listdir(src):\n",
    "#         s = os.path.join(src, item)\n",
    "#         d = os.path.join(dst, item)\n",
    "#         if os.path.isdir(s):\n",
    "#             shutil.copytree(s, d, symlinks, ignore)\n",
    "#         else:\n",
    "#             shutil.copy2(s, d)\n",
    "# copytree()\n",
    "# os.makedirs(\"MaskRCNN/newdataset\",exist_ok=True)\n",
    "# shutil.copytree('MaskRCNN/newdataset','dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ReNxoVC0mxwM",
    "outputId": "ab63600f-f33c-4773-adec-9f63b2876a39"
   },
   "outputs": [],
   "source": [
    "#!pip install keras==2.2.5\n",
    "#%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D7uGsj2Ho5M2",
    "outputId": "0f79e678-20df-4c82-969d-c5e722511cd6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 10:14:20.186403: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-17 10:14:20.742600: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nouman/anaconda3/envs/MRCNN/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-03-17 10:14:20.742627: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-17 10:14:22.220467: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nouman/anaconda3/envs/MRCNN/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-03-17 10:14:22.220589: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nouman/anaconda3/envs/MRCNN/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-03-17 10:14:22.220601: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/nouman/anaconda3/envs/MRCNN/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import skimage.draw\n",
    "import cv2\n",
    "from mrcnn.visualize import display_instances\n",
    "import matplotlib.pyplot as plt\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"wastedata-Mask_RCNN-multiple-classes/main/Mask_RCNN/\")\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import model as modellib, utils\n",
    "# Path to trained weights file\n",
    "COCO_WEIGHTS_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Directory to save logs and model checkpoints\n",
    "DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, \"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import skimage.draw\n",
    "from mrcnn.utils import Dataset\n",
    "\n",
    "class CatDogDataset(Dataset):\n",
    "    def load_dataset(self, dataset_dir, is_train=True):\n",
    "        # Add classes\n",
    "        self.add_class(\"catdog\", 1, \"cat\")\n",
    "        self.add_class(\"catdog\", 2, \"dog\")\n",
    "\n",
    "        # Load annotations from JSON file\n",
    "        with open(os.path.join(dataset_dir, \"annot/MRCNN_coco.json\")) as f:\n",
    "            annotations = json.load(f)\n",
    "\n",
    "        # Get a list of all image files\n",
    "        images_dir = dataset_dir + \"/images\"\n",
    "        image_files = sorted(os.listdir(images_dir))\n",
    "        image_ids = [i for i in range(len(image_files))]\n",
    "\n",
    "        # Set train or validation dataset\n",
    "        if is_train:\n",
    "            self.train_ids = image_ids\n",
    "        else:\n",
    "            self.val_ids = image_ids\n",
    "\n",
    "        # Add images and annotations\n",
    "        for i, image_id in enumerate(image_ids):\n",
    "            # Add image file\n",
    "            image_file = image_files[image_id]\n",
    "            image_path = images_dir + image_file\n",
    "            self.add_image(\"catdog\", image_id=image_id, path=image_path)\n",
    "\n",
    "            # Add annotation\n",
    "            image_annotations = annotations.get(image_file, [])\n",
    "            for annotation in image_annotations:\n",
    "                class_name = annotation['class_name']\n",
    "                if class_name not in self.class_info:\n",
    "                    continue\n",
    "                class_id = self.class_names.index(class_name)\n",
    "                mask = self.generate_mask(annotation)\n",
    "                if mask is not None:\n",
    "                    self.add_mask(\"catdog\", image_id=image_id, class_id=class_id, mask=mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "CEu6mNMSpKnn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/nouman/anaconda3/envs/MRCNN/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "WARNING:tensorflow:From /home/nouman/anaconda3/envs/MRCNN/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 10:14:31.040283: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nouman/anaconda3/envs/MRCNN/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-03-17 10:14:31.040312: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-17 10:14:31.040334: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (nouman-HP-EliteBook-840-G2): /proc/driver/nvidia/version does not exist\n",
      "2023-03-17 10:14:31.041040: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-17 10:14:31.135624: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import model as modellib\n",
    "from mrcnn import utils\n",
    "#from custom_dataset import CustomDataset\n",
    "import tensorflow.compat.v1 as tf\n",
    "# Define the configuration for the model\n",
    "class MyConfig(Config):\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"myconfig\"\n",
    "    # Adjust the number of classes based on your custom dataset\n",
    "    NUM_CLASSES = 2 + 1  # background + your classes\n",
    "    # Other configuration parameters\n",
    "    BACKBONE = \"resnet50\"\n",
    "    IMAGES_PER_GPU = 2\n",
    "    STEPS_PER_EPOCH = 100\n",
    "    VALIDATION_STEPS = 50\n",
    "\n",
    "# Load the pre-trained COCO weights into the model\n",
    "COCO_MODEL_PATH = \"/home/nouman/Downloads/training_batch/week5/mask_rcnn_coco.h5\"\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=MyConfig(), model_dir='./')\n",
    "model.load_weights(\"/home/nouman/Downloads/training_batch/week5/mask_rcnn_coco.h5\", by_name=True, \n",
    "                   exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "\n",
    "# # Replace the output layer of the model with a new output layer that matches the number of classes in your custom dataset\n",
    "# model.layers[-1].kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)\n",
    "# model.layers[-1].bias_initializer = tf.keras.initializers.Zeros()\n",
    "# model.layers[-1].units = MyConfig.NUM_CLASSES\n",
    "# model.layers[-1].activation = \"sigmoid\"\n",
    "\n",
    "# # Freeze the layers of the pre-trained model up to the last few layers, leaving the new output layer unfrozen\n",
    "# layers_to_freeze = 100\n",
    "# for i, layer in enumerate(model.layers):\n",
    "#     if i < layers_to_freeze:\n",
    "#         layer.trainable = False\n",
    "#     else:\n",
    "#         layer.trainable = True\n",
    "\n",
    "# Load the training and validation data\n",
    "dataset_train = CatDogDataset()\n",
    "dataset_train.load_dataset(\"/home/nouman/Downloads/training_batch/week5/catdog\")\n",
    "dataset_train.prepare()\n",
    "\n",
    "#dataset_val = CustomDataset()\n",
    "#dataset_val.load_dataset(\"path/to/validation/data\", \"val\")\n",
    "#dataset_val.prepare()\n",
    "\n",
    "# Train the model on your custom dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: ./myconfig20230317T1014/mask_rcnn_myconfig_{epoch:04d}.h5\n",
      "Selecting layers to train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 10:14:46.903136: W tensorflow/c/c_api.cc:291] Operation '{name:'range_1' id:7163 op device:{requested: '', assigned: ''} def:{{{node range_1}} = Range[Tidx=DT_INT32, _has_manual_control_dependencies=true](range_1/start, Rank_1, range_1/delta)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'ListWrapper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhead\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/training_batch/week5/mrcnn/model.py:2373\u001b[0m, in \u001b[0;36mMaskRCNN.train\u001b[0;34m(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)\u001b[0m\n\u001b[1;32m   2371\u001b[0m log(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoint Path: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_path))\n\u001b[1;32m   2372\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_trainable(layers)\n\u001b[0;32m-> 2373\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLEARNING_MOMENTUM\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2375\u001b[0m \u001b[38;5;66;03m# Work-around for Windows: Keras fails on Windows when using\u001b[39;00m\n\u001b[1;32m   2376\u001b[0m \u001b[38;5;66;03m# multiprocessing workers. See discussion here:\u001b[39;00m\n\u001b[1;32m   2377\u001b[0m \u001b[38;5;66;03m# https://github.com/matterport/Mask_RCNN/issues/13#issuecomment-353124009\u001b[39;00m\n\u001b[1;32m   2378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/Downloads/training_batch/week5/mrcnn/model.py:2194\u001b[0m, in \u001b[0;36mMaskRCNN.compile\u001b[0;34m(self, learning_rate, momentum)\u001b[0m\n\u001b[1;32m   2190\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2191\u001b[0m     loss \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2192\u001b[0m         tf\u001b[38;5;241m.\u001b[39mreduce_mean(layer\u001b[38;5;241m.\u001b[39moutput, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2193\u001b[0m         \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mLOSS_WEIGHTS\u001b[38;5;241m.\u001b[39mget(name, \u001b[38;5;241m1.\u001b[39m))\n\u001b[0;32m-> 2194\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2196\u001b[0m \u001b[38;5;66;03m# Add L2 Regularization\u001b[39;00m\n\u001b[1;32m   2197\u001b[0m \u001b[38;5;66;03m# Skip gamma and beta weights of batch normalization layers.\u001b[39;00m\n\u001b[1;32m   2198\u001b[0m reg_losses \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   2199\u001b[0m     keras\u001b[38;5;241m.\u001b[39mregularizers\u001b[38;5;241m.\u001b[39ml2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mWEIGHT_DECAY)(w) \u001b[38;5;241m/\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(tf\u001b[38;5;241m.\u001b[39msize(w), tf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m   2200\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeras_model\u001b[38;5;241m.\u001b[39mtrainable_weights\n\u001b[1;32m   2201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m w\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m w\u001b[38;5;241m.\u001b[39mname]\n",
      "File \u001b[0;32m~/anaconda3/envs/MRCNN/lib/python3.9/site-packages/keras/engine/base_layer_v1.py:1156\u001b[0m, in \u001b[0;36mLayer.add_loss\u001b[0;34m(self, losses, inputs)\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m symbolic_loss \u001b[38;5;129;01min\u001b[39;00m symbolic_losses:\n\u001b[1;32m   1155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_graph_network\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 1156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph_network_add_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43msymbolic_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1157\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1158\u001b[0m         \u001b[38;5;66;03m# Possible a loss was added in a Layer's `build`.\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_losses\u001b[38;5;241m.\u001b[39mappend(symbolic_loss)\n",
      "File \u001b[0;32m~/anaconda3/envs/MRCNN/lib/python3.9/site-packages/keras/engine/functional.py:1000\u001b[0m, in \u001b[0;36mFunctional._graph_network_add_loss\u001b[0;34m(self, symbolic_loss)\u001b[0m\n\u001b[1;32m    998\u001b[0m new_nodes\u001b[38;5;241m.\u001b[39mextend(add_loss_layer\u001b[38;5;241m.\u001b[39minbound_nodes)\n\u001b[1;32m    999\u001b[0m new_layers\u001b[38;5;241m.\u001b[39mappend(add_loss_layer)\n\u001b[0;32m-> 1000\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_insert_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_nodes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MRCNN/lib/python3.9/site-packages/keras/engine/functional.py:937\u001b[0m, in \u001b[0;36mFunctional._insert_layers\u001b[0;34m(self, layers, relevant_nodes)\u001b[0m\n\u001b[1;32m    934\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nodes_by_depth[depth]\u001b[38;5;241m.\u001b[39mappend(node)\n\u001b[1;32m    936\u001b[0m \u001b[38;5;66;03m# Insert layers and update other layer attrs.\u001b[39;00m\n\u001b[0;32m--> 937\u001b[0m layer_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_self_tracked_trackables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    938\u001b[0m deferred_layers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layers:\n",
      "File \u001b[0;32m~/anaconda3/envs/MRCNN/lib/python3.9/site-packages/tensorflow/python/trackable/data_structures.py:676\u001b[0m, in \u001b[0;36mListWrapper.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m   \u001b[38;5;66;03m# List wrappers need to compare like regular lists, and so like regular\u001b[39;00m\n\u001b[1;32m    675\u001b[0m   \u001b[38;5;66;03m# lists they don't belong in hash tables.\u001b[39;00m\n\u001b[0;32m--> 676\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munhashable type: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mListWrapper\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'ListWrapper'"
     ]
    }
   ],
   "source": [
    "model.train(dataset_train,dataset_train, epochs=10, layers=\"head\",learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotations1 = json.load(open(os.path.join(\"/home/nouman/Downloads/training_batch/week5/dataset/train\", \"cat_dog_annotations.json\")))\n",
    "#         # print(annotations1)\n",
    "# annotations = list(annotations1.values())\n",
    "# annotations[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotations[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotations[1][\"1.jpeg45602\"][\"regions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotations1 = json.load(open(os.path.join(\"/home/nouman/Downloads/training_batch/week5/dataset/train\", \"cat_dog_annotations.json\")))\n",
    "#         # print(annotations1)\n",
    "# annotation = list(annotations1.values())  # don't need the dict keys\n",
    "\n",
    "# # The VIA tool saves images in the JSON even if they don't have any\n",
    "# # annotations. Skip unannotated images.\n",
    "# annotations= []\n",
    "# for key in annotation[1].keys():\n",
    "#     annot = annotation[1][key][\"regions\"]\n",
    "#     annotations.append(annot)\n",
    "# #annotations = [a for a in annotations if a['regions']]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.jpeg\n",
      "2.jpeg\n",
      "3.jpeg\n",
      "4.jpeg\n",
      "5.jpeg\n",
      "6.jpeg\n",
      "7.jpeg\n"
     ]
    }
   ],
   "source": [
    "# for i,x in enumerate(annotation[1].keys()):\n",
    "#     print(annotation[1][x][\"filename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add images\n",
    "# for i,a in enumerate(annotations):\n",
    "#             # print(a)\n",
    "#             # Get the x, y coordinaets of points of the polygons that make up\n",
    "#             # the outline of each object instance. There are stores in the\n",
    "#             # shape_attributes (see json format above)\n",
    "#             polygons = [r['shape_attributes'] for r in annotations[i]] \n",
    "#             objects = [s['region_attributes']['class'] for s in annotations[i]]\n",
    "#             print(\"objects:\",objects)\n",
    "#             name_dict = {\"cat\": 1,\"dog\": 2} #,\"xyz\": 3}\n",
    "#             # key = tuple(name_dict)\n",
    "#             num_ids = [name_dict[a] for a in objects[i].keys()]\n",
    "     \n",
    "#             # num_ids = [int(n['Event']) for n in objects]\n",
    "#             # load_mask() needs the image size to convert polygons to masks.\n",
    "#             # Unfortunately, VIA doesn't include it in JSON, so we must read\n",
    "#             # the image. This is only managable since the dataset is tiny.\n",
    "#             print(\"numids\",num_ids)\n",
    "#             image_path = os.path.join(dataset_dir, a['filename'])\n",
    "#             image = skimage.io.imread(image_path)\n",
    "#             height, width = image.shape[:2]\n",
    "\n",
    "#             self.add_image(\n",
    "#                 \"object\",  ## for a single class just add the name here\n",
    "#                 image_id=a['filename'],  # use file name as a unique image id\n",
    "#                 path=image_path,\n",
    "#                 width=width, height=height,\n",
    "#                 polygons=polygons,\n",
    "#                 num_ids=num_ids\n",
    "#                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a in name_dict.keys():\n",
    "#     print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a in objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "XQWNIcbZo5j9"
   },
   "outputs": [],
   "source": [
    "# class CustomDataset(utils.Dataset):\n",
    "\n",
    "#     def load_custom(self, dataset_dir, subset):\n",
    "#         \"\"\"Load a subset of the Horse-Man dataset.\n",
    "#         dataset_dir: Root directory of the dataset.\n",
    "#         subset: Subset to load: train or val\n",
    "#         \"\"\"\n",
    "#         # Add classes. We have only one class to add.\n",
    "#         self.add_class(\"object\", 1, \"cat\")\n",
    "#         self.add_class(\"object\", 2, \"dog\")\n",
    "#         # self.add_class(\"object\", 3, \"xyz\") #likewise\n",
    "\n",
    "#         # Train or validation dataset?\n",
    "#         assert subset in [\"train\", \"val\"]\n",
    "#         dataset_dir = os.path.join(dataset_dir, subset)\n",
    "\n",
    "#         # Load annotations\n",
    "#         # VGG Image Annotator saves each image in the form:\n",
    "#         # { 'filename': '28503151_5b5b7ec140_b.jpg',\n",
    "#         #   'regions': {\n",
    "#         #       '0': {\n",
    "#         #           'region_attributes': {},\n",
    "#         #           'shape_attributes': {\n",
    "#         #               'all_points_x': [...],\n",
    "#         #               'all_points_y': [...],\n",
    "#         #               'name': 'polygon'}},\n",
    "#         #       ... more regions ...\n",
    "#         #   },\n",
    "#         #   'size': 100202\n",
    "#         # }\n",
    "#         # We mostly care about the x and y coordinates of each region\n",
    "#         annotations1 = json.load(open(os.path.join(dataset_dir, \"cat_dog_annotations.json\")))\n",
    "#         # print(annotations1)\n",
    "#         annotation = list(annotations1.values())  # don't need the dict keys\n",
    "\n",
    "#         # The VIA tool saves images in the JSON even if they don't have any\n",
    "#         # annotations. Skip unannotated images.\n",
    "#         annotations= []\n",
    "#         for key in annotation[1].keys():\n",
    "#             annot = annotation[1][key][\"regions\"]\n",
    "#             annotations.append(annot)\n",
    "#         #annotations = [a for a in annotations if a['regions']]\n",
    "        \n",
    "#         # Add images\n",
    "#         for i,a in enumerate(annotations):\n",
    "#             # print(a)\n",
    "#             # Get the x, y coordinaets of points of the polygons that make up\n",
    "#             # the outline of each object instance. There are stores in the\n",
    "#             # shape_attributes (see json format above)\n",
    "#             polygons = [r['shape_attributes'] for r in annotations[i]] \n",
    "#             objects = [s['region_attributes']['class'] for s in annotations[i]]\n",
    "#             print(\"objects:\",objects)\n",
    "#             name_dict = {\"cat\": 1,\"dog\": 2} #,\"xyz\": 3}\n",
    "#             # key = tuple(name_dict)\n",
    "#             num_ids = [name_dict[a] for a in objects[i].keys()]\n",
    "     \n",
    "#             num_ids = [int(n['Event']) for n in objects]\n",
    "#             # load_mask() needs the image size to convert polygons to masks.\n",
    "#             # Unfortunately, VIA doesn't include it in JSON, so we must read\n",
    "#             # the image. This is only managable since the dataset is tiny.\n",
    "#             #print(\"numids\",num_ids)\n",
    "#             image_path = os.path.join(dataset_dir, annotation[1][x][\"filename\"])\n",
    "#             image = skimage.io.imread(image_path)\n",
    "#             height, width = image.shape[:2]\n",
    "\n",
    "#             self.add_image(\n",
    "#                 \"object\",  ## for a single class just add the name here\n",
    "#                 image_id=annotation[1][x][\"filename\"],  # use file name as a unique image id\n",
    "#                 path=image_path,\n",
    "#                 width=width, height=height,\n",
    "#                 polygons=polygons,\n",
    "#                 num_ids=num_ids\n",
    "#                 )\n",
    "\n",
    "#     def load_mask(self, image_id):\n",
    "#         \"\"\"Generate instance masks for an image.\n",
    "#        Returns:\n",
    "#         masks: A bool array of shape [height, width, instance count] with\n",
    "#             one mask per instance.\n",
    "#         class_ids: a 1D array of class IDs of the instance masks.\n",
    "#         \"\"\"\n",
    "#         # If not a Horse/Man dataset image, delegate to parent class.\n",
    "#         image_info = self.image_info[image_id]\n",
    "#         if image_info[\"source\"] != \"object\":\n",
    "#             return super(self.__class__, self).load_mask(image_id)\n",
    "\n",
    "#         # Convert polygons to a bitmap mask of shape\n",
    "#         # [height, width, instance_count]\n",
    "#         info = self.image_info[image_id]\n",
    "#         if info[\"source\"] != \"object\":\n",
    "#             return super(self.__class__, self).load_mask(image_id)\n",
    "#         num_ids = info['num_ids']\n",
    "#         mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n",
    "#                         dtype=np.uint8)\n",
    "#         for i, p in enumerate(info[\"polygons\"]):\n",
    "#             # Get indexes of pixels inside the polygon and set them to 1\n",
    "#         \trr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n",
    "\n",
    "#         \tmask[rr, cc, i] = 1\n",
    "\n",
    "#         # Return mask, and array of class IDs of each instance. Since we have\n",
    "#         # one class ID only, we return an array of 1s\n",
    "#         # Map class names to class IDs.\n",
    "#         num_ids = np.array(num_ids, dtype=np.int32)\n",
    "#         return mask, num_ids #np.ones([mask.shape[-1]], dtype=np.int32)\n",
    "\n",
    "#     def image_reference(self, image_id):\n",
    "#         \"\"\"Return the path of the image.\"\"\"\n",
    "#         info = self.image_info[image_id]\n",
    "#         if info[\"source\"] == \"object\":\n",
    "#             return info[\"path\"]\n",
    "#         else:\n",
    "#             super(self.__class__, self).image_reference(image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "LxxH2PpklNli"
   },
   "outputs": [],
   "source": [
    "# def train(model):\n",
    "#     \"\"\"Train the model.\"\"\"\n",
    "#     # Training dataset.\n",
    "#     dataset_train = CustomDataset()\n",
    "#     dataset_train.load_custom(\"dataset\", \"train\")\n",
    "#     dataset_train.prepare()\n",
    "\n",
    "#     # Validation dataset\n",
    "#     dataset_val = CustomDataset()\n",
    "#     dataset_val.load_custom(\"dataset\", \"val\")\n",
    "#     dataset_val.prepare()\n",
    "\n",
    "#     # *** This training schedule is an example. Update to your needs ***\n",
    "#     # Since we're using a very small dataset, and starting from\n",
    "#     # COCO trained weights, we don't need to train too long. Also,\n",
    "#     # no need to train all layers, just the heads should do it.\n",
    "#     print(\"Training network heads\")\n",
    "#     model.train(dataset_train, dataset_val,\n",
    "#                 learning_rate=config.LEARNING_RATE,\n",
    "#                 epochs=10,\n",
    "#                 layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nouman/Downloads/training_batch/week5/wastedata-Mask_RCNN-multiple-classes/main/Mask_RCNN/logs'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEFAULT_LOGS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow\n",
    "# print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "id": "1dAoSlsApuWp",
    "outputId": "501d6033-be10-4c34-9099-d776d8ed0e9a"
   },
   "outputs": [],
   "source": [
    "# config = CustomConfig()\n",
    "# model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "#                                   model_dir=DEFAULT_LOGS_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights_path = COCO_WEIGHTS_PATH\n",
    "#         # Download weights file\n",
    "# if not os.path.exists(weights_path):\n",
    "#     utils.download_trained_weights(weights_path)\n",
    "\n",
    "# model.load_weights(weights_path, by_name=True, exclude=[\n",
    "#             \"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n",
    "#             \"mrcnn_bbox\", \"mrcnn_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "BbEIxAtPkOiw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objects: [{'dog': True}]\n",
      "numids [2]\n",
      "objects: [{'dog': True}, {'cat': True}]\n",
      "numids [1]\n",
      "objects: [{'cat': True}]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5991/2331490221.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_5991/467836156.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Training dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdataset_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_custom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdataset_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5991/2362698113.py\u001b[0m in \u001b[0;36mload_custom\u001b[0;34m(self, dataset_dir, subset)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mname_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"cat\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"dog\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m#,\"xyz\": 3}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;31m# key = tuple(name_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mnum_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mname_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;31m# num_ids = [int(n['Event']) for n in objects]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# train(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aj6EKBgckOlB"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# import random\n",
    "# import math\n",
    "# import re\n",
    "# import time\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "# import matplotlib.image as mpimg\n",
    "# # Root directory of the project\n",
    "# #ROOT_DIR = os.path.abspath(\"/\")\n",
    "\n",
    "# # Import Mask RCNN\n",
    "# sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "# from mrcnn import utils\n",
    "# from mrcnn import visualize\n",
    "# from mrcnn.visualize import display_images\n",
    "# import mrcnn.model as modellib\n",
    "# from mrcnn.model import log\n",
    "# %matplotlib inline\n",
    "# # Directory to save logs and trained model\n",
    "# MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "# # Path to Ballon trained weights\n",
    "# # You can download this file from the Releases page\n",
    "# # https://github.com/matterport/Mask_RCNN/releases\n",
    "# WEIGHTS_PATH = \"/content/wastedata-Mask_RCNN-multiple-classes/main/Mask_RCNN/logs/object20201209T0658/mask_rcnn_object_0010.h5\"  # TODO: update this path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "xLfFptu-kOnu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_META_SIZE                15\n",
      "IMAGE_MIN_DIM                  800\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           object\n",
      "NUM_CLASSES                    3\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# config = CustomConfig()\n",
    "# CUSTOM_DIR = os.path.join(ROOT_DIR, \"/content/dataset/\")\n",
    "# class InferenceConfig(config.__class__):\n",
    "#   # Run detection on one image at a time\n",
    "#   GPU_COUNT = 1\n",
    "#   IMAGES_PER_GPU = 1\n",
    "#   DETECTION_MIN_CONFIDENCE = 0.7\n",
    "# config = InferenceConfig()\n",
    "# config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "iO34muPRqxAb"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/dataset/val/cat_dog_annotations.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m CUSTOM_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m dataset \u001b[38;5;241m=\u001b[39m CustomDataset()\n\u001b[0;32m---> 14\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_custom\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCUSTOM_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Must call before using the dataset\u001b[39;00m\n\u001b[1;32m     16\u001b[0m dataset\u001b[38;5;241m.\u001b[39mprepare()\n",
      "Cell \u001b[0;32mIn[14], line 32\u001b[0m, in \u001b[0;36mCustomDataset.load_custom\u001b[0;34m(self, dataset_dir, subset)\u001b[0m\n\u001b[1;32m     15\u001b[0m dataset_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_dir, subset)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Load annotations\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# VGG Image Annotator saves each image in the form:\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# { 'filename': '28503151_5b5b7ec140_b.jpg',\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# }\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# We mostly care about the x and y coordinates of each region\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m annotations1 \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcat_dog_annotations.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# print(annotations1)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m annotation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(annotations1\u001b[38;5;241m.\u001b[39mvalues())  \u001b[38;5;66;03m# don't need the dict keys\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/MRCNN/lib/python3.9/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/dataset/val/cat_dog_annotations.json'"
     ]
    }
   ],
   "source": [
    "# # Device to load the neural network on. Useful if you're training a model on the same machine, in which case use CPU and leave the GPU for training.\n",
    "# DEVICE = \"/gpu:0\"  # /cpu:0 or /gpu:0\n",
    "# # Inspect the model in training or inference modes values: 'inference' or 'training'\n",
    "# TEST_MODE = \"inference\"\n",
    "# def get_ax(rows=1, cols=1, size=16):\n",
    "#   '''Return a Matplotlib Axes array to be used in all visualizations in the notebook. Provide a central point to control graph sizes. Adjust the size attribute to control \n",
    "#   how big to render images'''\n",
    "#   _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "#   return ax\n",
    "\n",
    "# # Load validation dataset\n",
    "# CUSTOM_DIR = \"/content/dataset\"\n",
    "# dataset = CustomDataset()\n",
    "# dataset.load_custom(CUSTOM_DIR, \"val\")\n",
    "# # Must call before using the dataset\n",
    "# dataset.prepare()\n",
    "# print(\"Images: {}\\nClasses: {}\".format(len(dataset.image_ids), dataset.class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7SwIv661qxCc"
   },
   "outputs": [],
   "source": [
    "# #LOAD MODEL\n",
    "# # Create model in inference mode\n",
    "# with tf.device(DEVICE):\n",
    "#   model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ykM96AxTqxFS"
   },
   "outputs": [],
   "source": [
    "# # Load COCO weights Or, load the last model you trained\n",
    "# weights_path = WEIGHTS_PATH\n",
    "# # Load weights\n",
    "# print(\"Loading weights \", weights_path)\n",
    "# model.load_weights(weights_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JVaXsTsYrOkH"
   },
   "outputs": [],
   "source": [
    "# #RUN DETECTION\n",
    "# image_id = random.choice(dataset.image_ids)\n",
    "# print(image_id)\n",
    "# image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "#   modellib.load_image_gt(dataset, config, image_id, use_mini_mask=False)\n",
    "# info = dataset.image_info[image_id]\n",
    "# print(\"image ID: {}.{} ({}) {}\".format(info[\"source\"], info[\"id\"], image_id,dataset.image_reference(image_id)))\n",
    "# # Run object detection\n",
    "# results = model.detect([image], verbose=1)\n",
    "# # Display results\n",
    "# x = get_ax(1)\n",
    "# r = results[0]\n",
    "# visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], dataset.class_names, r['scores'], ax=ax, title=\"Predictions\")\n",
    "# log(\"gt_class_id\", gt_class_id)\n",
    "# log(\"gt_bbox\", gt_bbox)\n",
    "# log(\"gt_mask\", gt_mask)\n",
    "# # This is for predicting images which are not present in dataset\n",
    "# path_to_new_image = '/content/dataset/test/unnamed.jpg'\n",
    "# image1 = mpimg.imread(path_to_new_image)\n",
    "# # Run object detection\n",
    "# print(len([image1]))\n",
    "# results1 = model.detect([image1], verbose=1)\n",
    "# # Display results\n",
    "# ax = get_ax(1)\n",
    "# r1 = results1[0]\n",
    "# visualize.display_instances(image1, r1['rois'], r1['masks'], r1['class_ids'],\n",
    "# dataset.class_names, r1['scores'], ax=ax, title=\"Predictions1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSYEnKwnrOmu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
